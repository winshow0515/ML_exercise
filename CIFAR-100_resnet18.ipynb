{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667a0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab3e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 1. 參數設定 -----\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e375177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:39<00:00, 4.29MB/s] \n"
     ]
    }
   ],
   "source": [
    "# ----- 2. 數據增強 & 載入 CIFAR-100 -----\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),   # 隨機裁切\n",
    "    transforms.RandomHorizontalFlip(),      # 隨機翻轉\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
    "                         (0.2675, 0.2565, 0.2761)) # CIFAR-100 mean/std\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
    "                         (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root=\"./data\", train=True,\n",
    "                                         download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root=\"./data\", train=False,\n",
    "                                        download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fef3f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 3. 建立 ResNet18 -----\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18(weights=None)  # 不載入 ImageNet 預訓練權重\n",
    "model.fc = nn.Linear(model.fc.in_features, 100)  # CIFAR-100 = 100 類別\n",
    "model = model.to(device)\n",
    "\n",
    "# ----- 4. 定義 Loss & Optimizer -----\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.2)  # 學習率衰減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7f18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 5. 訓練 & 測試函式 -----\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in trainloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {running_loss/len(trainloader):.3f} | \"\n",
    "          f\"Train Acc: {100.*correct/total:.2f}%\")\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss += criterion(outputs, targets).item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    print(f\"Test Loss: {loss/len(testloader):.3f} | Test Acc: {acc:.2f}%\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd2cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Train Loss: 3.999 | Train Acc: 9.53%\n",
      "Test Loss: 3.520 | Test Acc: 15.54%\n",
      "✅ Saved Best Model!\n",
      "Epoch [2/200] Train Loss: 3.370 | Train Acc: 17.84%\n",
      "Test Loss: 3.203 | Test Acc: 21.41%\n",
      "✅ Saved Best Model!\n",
      "Epoch [3/200] Train Loss: 3.079 | Train Acc: 23.24%\n",
      "Test Loss: 2.915 | Test Acc: 26.66%\n",
      "✅ Saved Best Model!\n",
      "Epoch [4/200] Train Loss: 2.850 | Train Acc: 27.50%\n",
      "Test Loss: 2.693 | Test Acc: 31.30%\n",
      "✅ Saved Best Model!\n",
      "Epoch [5/200] Train Loss: 2.698 | Train Acc: 30.71%\n",
      "Test Loss: 2.697 | Test Acc: 30.60%\n",
      "Epoch [6/200] Train Loss: 2.578 | Train Acc: 33.32%\n",
      "Test Loss: 2.477 | Test Acc: 35.13%\n",
      "✅ Saved Best Model!\n",
      "Epoch [7/200] Train Loss: 2.491 | Train Acc: 35.07%\n",
      "Test Loss: 2.477 | Test Acc: 36.04%\n",
      "✅ Saved Best Model!\n",
      "Epoch [8/200] Train Loss: 2.413 | Train Acc: 36.66%\n",
      "Test Loss: 2.476 | Test Acc: 35.96%\n",
      "Epoch [9/200] Train Loss: 2.354 | Train Acc: 37.92%\n",
      "Test Loss: 2.427 | Test Acc: 36.88%\n",
      "✅ Saved Best Model!\n",
      "Epoch [10/200] Train Loss: 2.289 | Train Acc: 39.32%\n",
      "Test Loss: 2.360 | Test Acc: 38.58%\n",
      "✅ Saved Best Model!\n",
      "Epoch [11/200] Train Loss: 2.252 | Train Acc: 39.87%\n",
      "Test Loss: 2.287 | Test Acc: 39.86%\n",
      "✅ Saved Best Model!\n",
      "Epoch [12/200] Train Loss: 2.202 | Train Acc: 41.21%\n",
      "Test Loss: 2.403 | Test Acc: 38.66%\n",
      "Epoch [13/200] Train Loss: 2.171 | Train Acc: 41.89%\n",
      "Test Loss: 2.239 | Test Acc: 41.36%\n",
      "✅ Saved Best Model!\n",
      "Epoch [14/200] Train Loss: 2.128 | Train Acc: 42.96%\n",
      "Test Loss: 2.180 | Test Acc: 42.10%\n",
      "✅ Saved Best Model!\n",
      "Epoch [15/200] Train Loss: 2.110 | Train Acc: 43.21%\n",
      "Test Loss: 2.249 | Test Acc: 41.83%\n",
      "Epoch [16/200] Train Loss: 2.082 | Train Acc: 43.80%\n",
      "Test Loss: 2.203 | Test Acc: 42.75%\n",
      "✅ Saved Best Model!\n",
      "Epoch [17/200] Train Loss: 2.051 | Train Acc: 44.62%\n",
      "Test Loss: 2.237 | Test Acc: 41.50%\n",
      "Epoch [18/200] Train Loss: 2.028 | Train Acc: 45.24%\n",
      "Test Loss: 2.186 | Test Acc: 42.24%\n",
      "Epoch [19/200] Train Loss: 2.027 | Train Acc: 45.23%\n",
      "Test Loss: 2.158 | Test Acc: 42.42%\n",
      "Epoch [20/200] Train Loss: 1.996 | Train Acc: 45.85%\n",
      "Test Loss: 2.098 | Test Acc: 44.02%\n",
      "✅ Saved Best Model!\n",
      "Epoch [21/200] Train Loss: 1.979 | Train Acc: 46.28%\n",
      "Test Loss: 2.180 | Test Acc: 43.08%\n",
      "Epoch [22/200] Train Loss: 1.965 | Train Acc: 46.53%\n",
      "Test Loss: 2.168 | Test Acc: 43.38%\n",
      "Epoch [23/200] Train Loss: 1.950 | Train Acc: 47.09%\n",
      "Test Loss: 2.302 | Test Acc: 41.22%\n",
      "Epoch [24/200] Train Loss: 1.944 | Train Acc: 46.96%\n",
      "Test Loss: 2.083 | Test Acc: 44.92%\n",
      "✅ Saved Best Model!\n",
      "Epoch [25/200] Train Loss: 1.926 | Train Acc: 47.59%\n",
      "Test Loss: 2.198 | Test Acc: 42.59%\n",
      "Epoch [26/200] Train Loss: 1.911 | Train Acc: 47.88%\n",
      "Test Loss: 2.097 | Test Acc: 44.72%\n",
      "Epoch [27/200] Train Loss: 1.897 | Train Acc: 48.06%\n",
      "Test Loss: 2.101 | Test Acc: 44.97%\n",
      "✅ Saved Best Model!\n",
      "Epoch [28/200] Train Loss: 1.894 | Train Acc: 48.32%\n",
      "Test Loss: 2.157 | Test Acc: 43.26%\n",
      "Epoch [29/200] Train Loss: 1.888 | Train Acc: 48.14%\n",
      "Test Loss: 2.111 | Test Acc: 44.38%\n",
      "Epoch [30/200] Train Loss: 1.879 | Train Acc: 48.51%\n",
      "Test Loss: 2.019 | Test Acc: 46.34%\n",
      "✅ Saved Best Model!\n",
      "Epoch [31/200] Train Loss: 1.867 | Train Acc: 49.00%\n",
      "Test Loss: 2.108 | Test Acc: 45.19%\n",
      "Epoch [32/200] Train Loss: 1.859 | Train Acc: 49.08%\n",
      "Test Loss: 2.291 | Test Acc: 42.27%\n",
      "Epoch [33/200] Train Loss: 1.854 | Train Acc: 49.13%\n",
      "Test Loss: 2.135 | Test Acc: 43.65%\n",
      "Epoch [34/200] Train Loss: 1.850 | Train Acc: 49.16%\n",
      "Test Loss: 2.082 | Test Acc: 45.12%\n",
      "Epoch [35/200] Train Loss: 1.853 | Train Acc: 49.10%\n",
      "Test Loss: 2.071 | Test Acc: 45.67%\n",
      "Epoch [36/200] Train Loss: 1.827 | Train Acc: 49.79%\n",
      "Test Loss: 1.979 | Test Acc: 47.62%\n",
      "✅ Saved Best Model!\n",
      "Epoch [37/200] Train Loss: 1.826 | Train Acc: 49.92%\n",
      "Test Loss: 2.095 | Test Acc: 45.40%\n",
      "Epoch [38/200] Train Loss: 1.828 | Train Acc: 49.67%\n",
      "Test Loss: 2.150 | Test Acc: 44.78%\n",
      "Epoch [39/200] Train Loss: 1.812 | Train Acc: 50.08%\n",
      "Test Loss: 2.101 | Test Acc: 45.09%\n",
      "Epoch [40/200] Train Loss: 1.823 | Train Acc: 49.87%\n",
      "Test Loss: 2.077 | Test Acc: 45.85%\n",
      "Epoch [41/200] Train Loss: 1.819 | Train Acc: 50.04%\n",
      "Test Loss: 2.102 | Test Acc: 45.65%\n",
      "Epoch [42/200] Train Loss: 1.803 | Train Acc: 50.69%\n",
      "Test Loss: 2.021 | Test Acc: 47.51%\n",
      "Epoch [43/200] Train Loss: 1.799 | Train Acc: 50.53%\n",
      "Test Loss: 2.207 | Test Acc: 44.77%\n",
      "Epoch [44/200] Train Loss: 1.802 | Train Acc: 50.55%\n",
      "Test Loss: 1.999 | Test Acc: 47.20%\n",
      "Epoch [45/200] Train Loss: 1.787 | Train Acc: 50.63%\n",
      "Test Loss: 2.042 | Test Acc: 46.75%\n",
      "Epoch [46/200] Train Loss: 1.795 | Train Acc: 50.47%\n",
      "Test Loss: 2.099 | Test Acc: 45.55%\n",
      "Epoch [47/200] Train Loss: 1.787 | Train Acc: 50.81%\n",
      "Test Loss: 2.139 | Test Acc: 44.89%\n",
      "Epoch [48/200] Train Loss: 1.783 | Train Acc: 50.82%\n",
      "Test Loss: 2.058 | Test Acc: 45.74%\n",
      "Epoch [49/200] Train Loss: 1.776 | Train Acc: 51.04%\n",
      "Test Loss: 2.125 | Test Acc: 43.93%\n",
      "Epoch [50/200] Train Loss: 1.777 | Train Acc: 50.96%\n",
      "Test Loss: 2.048 | Test Acc: 46.19%\n",
      "Epoch [51/200] Train Loss: 1.779 | Train Acc: 50.86%\n",
      "Test Loss: 2.077 | Test Acc: 45.31%\n",
      "Epoch [52/200] Train Loss: 1.775 | Train Acc: 50.95%\n",
      "Test Loss: 2.015 | Test Acc: 46.57%\n",
      "Epoch [53/200] Train Loss: 1.768 | Train Acc: 51.00%\n",
      "Test Loss: 2.142 | Test Acc: 44.68%\n",
      "Epoch [54/200] Train Loss: 1.768 | Train Acc: 51.31%\n",
      "Test Loss: 1.944 | Test Acc: 47.86%\n",
      "✅ Saved Best Model!\n",
      "Epoch [55/200] Train Loss: 1.760 | Train Acc: 51.44%\n",
      "Test Loss: 2.169 | Test Acc: 44.12%\n",
      "Epoch [56/200] Train Loss: 1.767 | Train Acc: 51.39%\n",
      "Test Loss: 2.089 | Test Acc: 45.77%\n",
      "Epoch [57/200] Train Loss: 1.765 | Train Acc: 51.39%\n",
      "Test Loss: 2.166 | Test Acc: 43.93%\n",
      "Epoch [58/200] Train Loss: 1.771 | Train Acc: 51.38%\n",
      "Test Loss: 2.038 | Test Acc: 46.75%\n",
      "Epoch [59/200] Train Loss: 1.765 | Train Acc: 51.35%\n",
      "Test Loss: 2.157 | Test Acc: 44.09%\n",
      "Epoch [60/200] Train Loss: 1.751 | Train Acc: 51.42%\n",
      "Test Loss: 1.983 | Test Acc: 47.96%\n",
      "✅ Saved Best Model!\n",
      "Epoch [61/200] Train Loss: 1.251 | Train Acc: 64.13%\n",
      "Test Loss: 1.521 | Test Acc: 58.76%\n",
      "✅ Saved Best Model!\n",
      "Epoch [62/200] Train Loss: 1.080 | Train Acc: 68.76%\n",
      "Test Loss: 1.523 | Test Acc: 58.57%\n",
      "Epoch [63/200] Train Loss: 1.018 | Train Acc: 70.09%\n",
      "Test Loss: 1.536 | Test Acc: 58.26%\n",
      "Epoch [64/200] Train Loss: 0.958 | Train Acc: 71.67%\n",
      "Test Loss: 1.585 | Test Acc: 58.28%\n",
      "Epoch [65/200] Train Loss: 0.925 | Train Acc: 72.31%\n",
      "Test Loss: 1.579 | Test Acc: 58.26%\n",
      "Epoch [66/200] Train Loss: 0.900 | Train Acc: 72.92%\n",
      "Test Loss: 1.563 | Test Acc: 58.80%\n",
      "✅ Saved Best Model!\n",
      "Epoch [67/200] Train Loss: 0.879 | Train Acc: 73.64%\n",
      "Test Loss: 1.677 | Test Acc: 57.05%\n",
      "Epoch [68/200] Train Loss: 0.858 | Train Acc: 74.07%\n",
      "Test Loss: 1.627 | Test Acc: 57.95%\n",
      "Epoch [69/200] Train Loss: 0.849 | Train Acc: 74.41%\n",
      "Test Loss: 1.657 | Test Acc: 57.55%\n",
      "Epoch [70/200] Train Loss: 0.836 | Train Acc: 74.54%\n",
      "Test Loss: 1.674 | Test Acc: 57.21%\n",
      "Epoch [71/200] Train Loss: 0.829 | Train Acc: 74.64%\n",
      "Test Loss: 1.678 | Test Acc: 57.57%\n",
      "Epoch [72/200] Train Loss: 0.822 | Train Acc: 74.87%\n",
      "Test Loss: 1.756 | Test Acc: 55.90%\n",
      "Epoch [73/200] Train Loss: 0.811 | Train Acc: 75.17%\n",
      "Test Loss: 1.714 | Test Acc: 57.28%\n",
      "Epoch [74/200] Train Loss: 0.794 | Train Acc: 75.79%\n",
      "Test Loss: 1.758 | Test Acc: 55.87%\n",
      "Epoch [75/200] Train Loss: 0.777 | Train Acc: 76.25%\n",
      "Test Loss: 1.787 | Test Acc: 56.10%\n",
      "Epoch [76/200] Train Loss: 0.787 | Train Acc: 75.81%\n",
      "Test Loss: 1.839 | Test Acc: 55.41%\n",
      "Epoch [77/200] Train Loss: 0.771 | Train Acc: 76.17%\n",
      "Test Loss: 1.804 | Test Acc: 56.41%\n",
      "Epoch [78/200] Train Loss: 0.757 | Train Acc: 76.56%\n",
      "Test Loss: 1.793 | Test Acc: 55.91%\n",
      "Epoch [79/200] Train Loss: 0.748 | Train Acc: 76.85%\n",
      "Test Loss: 1.885 | Test Acc: 55.00%\n",
      "Epoch [80/200] Train Loss: 0.729 | Train Acc: 77.67%\n",
      "Test Loss: 1.849 | Test Acc: 55.58%\n",
      "Epoch [81/200] Train Loss: 0.735 | Train Acc: 77.21%\n",
      "Test Loss: 1.886 | Test Acc: 55.32%\n",
      "Epoch [82/200] Train Loss: 0.716 | Train Acc: 77.88%\n",
      "Test Loss: 1.840 | Test Acc: 56.28%\n",
      "Epoch [83/200] Train Loss: 0.712 | Train Acc: 77.91%\n",
      "Test Loss: 1.837 | Test Acc: 55.29%\n",
      "Epoch [84/200] Train Loss: 0.676 | Train Acc: 79.24%\n",
      "Test Loss: 1.885 | Test Acc: 55.59%\n",
      "Epoch [85/200] Train Loss: 0.690 | Train Acc: 78.67%\n",
      "Test Loss: 1.852 | Test Acc: 55.62%\n",
      "Epoch [86/200] Train Loss: 0.685 | Train Acc: 78.61%\n",
      "Test Loss: 1.863 | Test Acc: 55.87%\n",
      "Epoch [87/200] Train Loss: 0.667 | Train Acc: 79.17%\n",
      "Test Loss: 1.939 | Test Acc: 55.05%\n",
      "Epoch [88/200] Train Loss: 0.667 | Train Acc: 79.24%\n",
      "Test Loss: 1.860 | Test Acc: 55.86%\n",
      "Epoch [89/200] Train Loss: 0.638 | Train Acc: 80.46%\n",
      "Test Loss: 1.967 | Test Acc: 54.59%\n",
      "Epoch [90/200] Train Loss: 0.642 | Train Acc: 79.97%\n",
      "Test Loss: 1.959 | Test Acc: 54.51%\n",
      "Epoch [91/200] Train Loss: 0.634 | Train Acc: 80.24%\n",
      "Test Loss: 1.870 | Test Acc: 56.10%\n",
      "Epoch [92/200] Train Loss: 0.627 | Train Acc: 80.35%\n",
      "Test Loss: 2.018 | Test Acc: 53.33%\n",
      "Epoch [93/200] Train Loss: 0.619 | Train Acc: 80.66%\n",
      "Test Loss: 1.910 | Test Acc: 55.17%\n",
      "Epoch [94/200] Train Loss: 0.610 | Train Acc: 80.88%\n",
      "Test Loss: 1.937 | Test Acc: 55.13%\n",
      "Epoch [95/200] Train Loss: 0.609 | Train Acc: 81.19%\n",
      "Test Loss: 1.932 | Test Acc: 54.98%\n",
      "Epoch [96/200] Train Loss: 0.590 | Train Acc: 81.32%\n",
      "Test Loss: 2.054 | Test Acc: 54.00%\n",
      "Epoch [97/200] Train Loss: 0.594 | Train Acc: 81.42%\n",
      "Test Loss: 1.989 | Test Acc: 54.88%\n",
      "Epoch [98/200] Train Loss: 0.588 | Train Acc: 81.70%\n",
      "Test Loss: 1.975 | Test Acc: 55.10%\n",
      "Epoch [99/200] Train Loss: 0.570 | Train Acc: 82.32%\n",
      "Test Loss: 2.004 | Test Acc: 54.73%\n",
      "Epoch [100/200] Train Loss: 0.577 | Train Acc: 81.94%\n",
      "Test Loss: 2.001 | Test Acc: 54.33%\n",
      "Epoch [101/200] Train Loss: 0.566 | Train Acc: 82.29%\n",
      "Test Loss: 1.976 | Test Acc: 55.45%\n",
      "Epoch [102/200] Train Loss: 0.562 | Train Acc: 82.27%\n",
      "Test Loss: 1.975 | Test Acc: 54.75%\n",
      "Epoch [103/200] Train Loss: 0.559 | Train Acc: 82.40%\n",
      "Test Loss: 2.036 | Test Acc: 54.27%\n",
      "Epoch [104/200] Train Loss: 0.560 | Train Acc: 82.49%\n",
      "Test Loss: 2.125 | Test Acc: 53.42%\n",
      "Epoch [105/200] Train Loss: 0.540 | Train Acc: 83.03%\n",
      "Test Loss: 2.035 | Test Acc: 54.43%\n",
      "Epoch [106/200] Train Loss: 0.545 | Train Acc: 82.99%\n",
      "Test Loss: 2.025 | Test Acc: 54.44%\n",
      "Epoch [107/200] Train Loss: 0.541 | Train Acc: 83.04%\n",
      "Test Loss: 2.092 | Test Acc: 53.76%\n",
      "Epoch [108/200] Train Loss: 0.529 | Train Acc: 83.49%\n",
      "Test Loss: 2.020 | Test Acc: 54.75%\n",
      "Epoch [109/200] Train Loss: 0.543 | Train Acc: 82.99%\n",
      "Test Loss: 2.010 | Test Acc: 55.12%\n",
      "Epoch [110/200] Train Loss: 0.527 | Train Acc: 83.44%\n",
      "Test Loss: 2.023 | Test Acc: 54.89%\n",
      "Epoch [111/200] Train Loss: 0.520 | Train Acc: 83.91%\n",
      "Test Loss: 2.055 | Test Acc: 54.20%\n",
      "Epoch [112/200] Train Loss: 0.523 | Train Acc: 83.64%\n",
      "Test Loss: 2.035 | Test Acc: 54.48%\n",
      "Epoch [113/200] Train Loss: 0.526 | Train Acc: 83.44%\n",
      "Test Loss: 2.017 | Test Acc: 54.84%\n",
      "Epoch [114/200] Train Loss: 0.507 | Train Acc: 84.26%\n",
      "Test Loss: 2.019 | Test Acc: 54.20%\n",
      "Epoch [115/200] Train Loss: 0.504 | Train Acc: 84.23%\n",
      "Test Loss: 2.113 | Test Acc: 53.92%\n",
      "Epoch [116/200] Train Loss: 0.506 | Train Acc: 84.04%\n",
      "Test Loss: 2.072 | Test Acc: 54.43%\n",
      "Epoch [117/200] Train Loss: 0.508 | Train Acc: 83.93%\n",
      "Test Loss: 2.097 | Test Acc: 54.14%\n",
      "Epoch [118/200] Train Loss: 0.497 | Train Acc: 84.41%\n",
      "Test Loss: 2.057 | Test Acc: 54.94%\n",
      "Epoch [119/200] Train Loss: 0.503 | Train Acc: 84.27%\n",
      "Test Loss: 2.066 | Test Acc: 54.57%\n",
      "Epoch [120/200] Train Loss: 0.484 | Train Acc: 84.89%\n",
      "Test Loss: 2.110 | Test Acc: 54.38%\n",
      "Epoch [121/200] Train Loss: 0.235 | Train Acc: 93.50%\n",
      "Test Loss: 1.765 | Test Acc: 59.95%\n",
      "✅ Saved Best Model!\n",
      "Epoch [122/200] Train Loss: 0.141 | Train Acc: 96.64%\n",
      "Test Loss: 1.779 | Test Acc: 59.93%\n",
      "Epoch [123/200] Train Loss: 0.115 | Train Acc: 97.39%\n",
      "Test Loss: 1.782 | Test Acc: 60.19%\n",
      "✅ Saved Best Model!\n",
      "Epoch [124/200] Train Loss: 0.099 | Train Acc: 97.76%\n",
      "Test Loss: 1.796 | Test Acc: 60.39%\n",
      "✅ Saved Best Model!\n",
      "Epoch [125/200] Train Loss: 0.084 | Train Acc: 98.27%\n",
      "Test Loss: 1.805 | Test Acc: 60.17%\n",
      "Epoch [126/200] Train Loss: 0.078 | Train Acc: 98.49%\n",
      "Test Loss: 1.799 | Test Acc: 60.62%\n",
      "✅ Saved Best Model!\n",
      "Epoch [127/200] Train Loss: 0.071 | Train Acc: 98.59%\n",
      "Test Loss: 1.812 | Test Acc: 60.55%\n",
      "Epoch [128/200] Train Loss: 0.065 | Train Acc: 98.72%\n",
      "Test Loss: 1.804 | Test Acc: 60.49%\n",
      "Epoch [129/200] Train Loss: 0.059 | Train Acc: 98.91%\n",
      "Test Loss: 1.838 | Test Acc: 60.43%\n",
      "Epoch [130/200] Train Loss: 0.056 | Train Acc: 99.00%\n",
      "Test Loss: 1.835 | Test Acc: 60.50%\n",
      "Epoch [131/200] Train Loss: 0.053 | Train Acc: 99.05%\n",
      "Test Loss: 1.830 | Test Acc: 60.46%\n",
      "Epoch [132/200] Train Loss: 0.050 | Train Acc: 99.14%\n",
      "Test Loss: 1.843 | Test Acc: 60.54%\n",
      "Epoch [133/200] Train Loss: 0.049 | Train Acc: 99.15%\n",
      "Test Loss: 1.842 | Test Acc: 60.59%\n",
      "Epoch [134/200] Train Loss: 0.046 | Train Acc: 99.21%\n",
      "Test Loss: 1.840 | Test Acc: 60.64%\n",
      "✅ Saved Best Model!\n",
      "Epoch [135/200] Train Loss: 0.043 | Train Acc: 99.33%\n",
      "Test Loss: 1.842 | Test Acc: 60.90%\n",
      "✅ Saved Best Model!\n",
      "Epoch [136/200] Train Loss: 0.043 | Train Acc: 99.28%\n",
      "Test Loss: 1.856 | Test Acc: 60.55%\n",
      "Epoch [137/200] Train Loss: 0.041 | Train Acc: 99.32%\n",
      "Test Loss: 1.854 | Test Acc: 60.81%\n",
      "Epoch [138/200] Train Loss: 0.039 | Train Acc: 99.35%\n",
      "Test Loss: 1.849 | Test Acc: 60.54%\n",
      "Epoch [139/200] Train Loss: 0.037 | Train Acc: 99.42%\n",
      "Test Loss: 1.844 | Test Acc: 60.82%\n",
      "Epoch [140/200] Train Loss: 0.036 | Train Acc: 99.46%\n",
      "Test Loss: 1.855 | Test Acc: 60.59%\n",
      "Epoch [141/200] Train Loss: 0.036 | Train Acc: 99.45%\n",
      "Test Loss: 1.850 | Test Acc: 60.77%\n",
      "Epoch [142/200] Train Loss: 0.035 | Train Acc: 99.48%\n",
      "Test Loss: 1.863 | Test Acc: 60.64%\n",
      "Epoch [143/200] Train Loss: 0.034 | Train Acc: 99.49%\n",
      "Test Loss: 1.854 | Test Acc: 60.80%\n",
      "Epoch [144/200] Train Loss: 0.034 | Train Acc: 99.49%\n",
      "Test Loss: 1.853 | Test Acc: 60.52%\n",
      "Epoch [145/200] Train Loss: 0.032 | Train Acc: 99.56%\n",
      "Test Loss: 1.854 | Test Acc: 61.06%\n",
      "✅ Saved Best Model!\n",
      "Epoch [146/200] Train Loss: 0.032 | Train Acc: 99.55%\n",
      "Test Loss: 1.853 | Test Acc: 61.08%\n",
      "✅ Saved Best Model!\n",
      "Epoch [147/200] Train Loss: 0.032 | Train Acc: 99.49%\n",
      "Test Loss: 1.851 | Test Acc: 60.86%\n",
      "Epoch [148/200] Train Loss: 0.030 | Train Acc: 99.56%\n",
      "Test Loss: 1.851 | Test Acc: 61.16%\n",
      "✅ Saved Best Model!\n",
      "Epoch [149/200] Train Loss: 0.029 | Train Acc: 99.63%\n",
      "Test Loss: 1.845 | Test Acc: 60.88%\n",
      "Epoch [150/200] Train Loss: 0.029 | Train Acc: 99.61%\n",
      "Test Loss: 1.848 | Test Acc: 61.11%\n",
      "Epoch [151/200] Train Loss: 0.030 | Train Acc: 99.57%\n",
      "Test Loss: 1.843 | Test Acc: 61.20%\n",
      "✅ Saved Best Model!\n",
      "Epoch [152/200] Train Loss: 0.030 | Train Acc: 99.60%\n",
      "Test Loss: 1.851 | Test Acc: 60.99%\n",
      "Epoch [153/200] Train Loss: 0.029 | Train Acc: 99.62%\n",
      "Test Loss: 1.851 | Test Acc: 60.93%\n",
      "Epoch [154/200] Train Loss: 0.027 | Train Acc: 99.64%\n",
      "Test Loss: 1.856 | Test Acc: 60.83%\n",
      "Epoch [155/200] Train Loss: 0.029 | Train Acc: 99.61%\n",
      "Test Loss: 1.848 | Test Acc: 60.54%\n",
      "Epoch [156/200] Train Loss: 0.027 | Train Acc: 99.68%\n",
      "Test Loss: 1.843 | Test Acc: 60.86%\n",
      "Epoch [157/200] Train Loss: 0.028 | Train Acc: 99.64%\n",
      "Test Loss: 1.843 | Test Acc: 61.12%\n",
      "Epoch [158/200] Train Loss: 0.027 | Train Acc: 99.63%\n",
      "Test Loss: 1.836 | Test Acc: 60.88%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m best_acc = \u001b[32m0\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     acc = test(epoch)\n\u001b[32m      6\u001b[39m     scheduler.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(epoch)\u001b[39m\n\u001b[32m      5\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m      6\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:494\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:427\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1172\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1165\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1171\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1172\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1174\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\context.py:337\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\popen_spawn_win32.py:97\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     96\u001b[39m     reduction.dump(prep_data, to_child)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[43mreduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     99\u001b[39m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winsh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\reduction.py:60\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, file, protocol)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump\u001b[39m(obj, file, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ----- 6. 主迴圈 -----\n",
    "best_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch)\n",
    "    acc = test(epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "    # 儲存最佳模型\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), \"resnet18_cifar100.pth\")\n",
    "        print(\"✅ Saved Best Model!\")\n",
    "\n",
    "print(f\"\\n🎉 Training Finished! Best Test Acc: {best_acc:.2f}%\")\n",
    "\n",
    "\n",
    "'''\n",
    "Epoch [158/200] Train Loss: 0.027 | Train Acc: 99.63%\n",
    "Test Loss: 1.836 | Test Acc: 60.88%\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
